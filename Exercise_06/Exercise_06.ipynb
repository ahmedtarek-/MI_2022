{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a8a7bc3-d0ac-413c-9f65-398fd64381af",
   "metadata": {},
   "source": [
    "# MI\n",
    "**_Ahmed - Luisa - Myria_**\n",
    "\n",
    "## Exercise H6.1 (Convolutional Neural Network)\n",
    "\n",
    "\n",
    "### 0. Setup and loading MNIST data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24b572f8-d809-467a-9b52-a06013f6c29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b2d3b47-771a-4e3d-a30e-1cb118185bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784), (60000, 10), (10000, 10))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Importing the MNIST dataset and reshaping it so every image is a 784 vector\n",
    "(X_train, y_train), (X_holdout, y_holdout) = mnist.load_data()\n",
    "\n",
    "train_dataset_size = X_train.shape[0]\n",
    "holdout_dataset_size = X_holdout.shape[0]\n",
    "\n",
    "image_height = X_train.shape[1]\n",
    "image_width = X_train.shape[2]\n",
    "\n",
    "X_train = X_train.reshape(train_dataset_size, (image_height*image_width))\n",
    "X_holdout = X_holdout.reshape(holdout_dataset_size, (image_height*image_width))\n",
    "\n",
    "# 2. Change ys to be array of 1s and 0s\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_holdout = to_categorical(y_holdout, num_classes=10)\n",
    "\n",
    "X_train.shape, X_holdout.shape, y_train.shape, y_holdout.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2c8dd6-9ae0-4f18-8c16-a52087ba428f",
   "metadata": {},
   "source": [
    "### 1. Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30ac314a-e265-4f16-9701-d5e997a2c400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                7850      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# 1. Define the model\n",
    "num_of_nodes = 10\n",
    "xi      = Input(shape=(image_height*image_width,))   # Input size (784 pixels)\n",
    "xo      = Dense(num_of_nodes)(xi)                    # Defines a linear model [by default one more dimension is added for bias]\n",
    "yo      = Activation('softmax')(xo)                  # Defining activation function to be softmax\n",
    "model   = Model(inputs=[xi], outputs=[yo])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48af73c5-a76e-43e4-a22d-ab666fc94171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1433.4808 - accuracy: 0.8981 - val_loss: 2060.4275 - val_accuracy: 0.8967\n",
      "Epoch 2/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1384.6316 - accuracy: 0.8997 - val_loss: 2073.8347 - val_accuracy: 0.8975\n",
      "Epoch 3/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1453.3243 - accuracy: 0.8985 - val_loss: 2354.6958 - val_accuracy: 0.8793\n",
      "Epoch 4/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1404.5942 - accuracy: 0.9014 - val_loss: 2124.0735 - val_accuracy: 0.8925\n",
      "Epoch 5/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1394.8992 - accuracy: 0.8999 - val_loss: 1939.9434 - val_accuracy: 0.9044\n",
      "Epoch 6/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1389.6921 - accuracy: 0.8999 - val_loss: 2364.4607 - val_accuracy: 0.8818\n",
      "Epoch 7/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1442.8169 - accuracy: 0.8972 - val_loss: 2094.8911 - val_accuracy: 0.8873\n",
      "Epoch 8/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1449.9888 - accuracy: 0.8984 - val_loss: 2503.3760 - val_accuracy: 0.8732\n",
      "Epoch 9/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1393.7225 - accuracy: 0.8988 - val_loss: 2061.7192 - val_accuracy: 0.8983\n",
      "Epoch 10/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1399.9882 - accuracy: 0.8987 - val_loss: 2426.3301 - val_accuracy: 0.8708\n",
      "Epoch 11/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1387.8654 - accuracy: 0.9005 - val_loss: 2034.7726 - val_accuracy: 0.8938\n",
      "Epoch 12/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1388.2281 - accuracy: 0.8996 - val_loss: 2073.3738 - val_accuracy: 0.8990\n",
      "Epoch 13/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1459.6036 - accuracy: 0.8978 - val_loss: 2181.4636 - val_accuracy: 0.8930\n",
      "Epoch 14/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1398.0690 - accuracy: 0.8983 - val_loss: 1948.9554 - val_accuracy: 0.8938\n",
      "Epoch 15/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1377.9147 - accuracy: 0.8988 - val_loss: 2149.1206 - val_accuracy: 0.8937\n",
      "Epoch 16/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1364.0242 - accuracy: 0.8990 - val_loss: 2193.7793 - val_accuracy: 0.8782\n",
      "Epoch 17/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1351.8724 - accuracy: 0.8994 - val_loss: 2192.3853 - val_accuracy: 0.8823\n",
      "Epoch 18/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1410.5883 - accuracy: 0.8994 - val_loss: 2386.9043 - val_accuracy: 0.8801\n",
      "Epoch 19/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1363.4883 - accuracy: 0.8993 - val_loss: 2056.2654 - val_accuracy: 0.8927\n",
      "Epoch 20/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1415.5775 - accuracy: 0.8979 - val_loss: 2080.2507 - val_accuracy: 0.8926\n",
      "Epoch 21/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1392.0948 - accuracy: 0.8978 - val_loss: 2255.5337 - val_accuracy: 0.8865\n",
      "Epoch 22/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1374.1938 - accuracy: 0.8997 - val_loss: 2385.2849 - val_accuracy: 0.8792\n",
      "Epoch 23/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1401.5608 - accuracy: 0.8986 - val_loss: 2300.9229 - val_accuracy: 0.8830\n",
      "Epoch 24/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1377.2118 - accuracy: 0.8980 - val_loss: 2180.8545 - val_accuracy: 0.8855\n",
      "Epoch 25/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1413.4745 - accuracy: 0.8993 - val_loss: 2291.0913 - val_accuracy: 0.8824\n",
      "Epoch 26/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1381.0205 - accuracy: 0.8989 - val_loss: 2023.4854 - val_accuracy: 0.8970\n",
      "Epoch 27/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1405.0259 - accuracy: 0.8985 - val_loss: 2121.5002 - val_accuracy: 0.8897\n",
      "Epoch 28/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1362.5291 - accuracy: 0.9003 - val_loss: 2108.5298 - val_accuracy: 0.9004\n",
      "Epoch 29/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1366.2338 - accuracy: 0.9008 - val_loss: 2096.8489 - val_accuracy: 0.8886\n",
      "Epoch 30/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1358.2584 - accuracy: 0.9004 - val_loss: 1977.0162 - val_accuracy: 0.9019\n",
      "Epoch 31/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1411.8214 - accuracy: 0.8987 - val_loss: 2038.2675 - val_accuracy: 0.8954\n",
      "Epoch 32/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1370.6940 - accuracy: 0.8997 - val_loss: 2119.2439 - val_accuracy: 0.8874\n",
      "Epoch 33/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1460.0054 - accuracy: 0.8984 - val_loss: 2318.6753 - val_accuracy: 0.8785\n",
      "Epoch 34/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1393.1333 - accuracy: 0.8991 - val_loss: 1935.7271 - val_accuracy: 0.9053\n",
      "Epoch 35/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1393.6128 - accuracy: 0.8990 - val_loss: 2049.7566 - val_accuracy: 0.8987\n",
      "Epoch 36/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1341.5374 - accuracy: 0.8997 - val_loss: 2158.5359 - val_accuracy: 0.8805\n",
      "Epoch 37/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1357.5280 - accuracy: 0.9011 - val_loss: 1953.9110 - val_accuracy: 0.8946\n",
      "Epoch 38/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1386.9119 - accuracy: 0.9000 - val_loss: 2225.6001 - val_accuracy: 0.8887\n",
      "Epoch 39/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1338.7090 - accuracy: 0.9005 - val_loss: 2184.6992 - val_accuracy: 0.8866\n",
      "Epoch 40/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1367.0833 - accuracy: 0.8996 - val_loss: 2165.1331 - val_accuracy: 0.8869\n",
      "Epoch 41/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1333.4366 - accuracy: 0.9001 - val_loss: 2692.3376 - val_accuracy: 0.8519\n",
      "Epoch 42/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1399.7780 - accuracy: 0.8983 - val_loss: 2231.0815 - val_accuracy: 0.8864\n",
      "Epoch 43/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1353.7679 - accuracy: 0.9005 - val_loss: 1939.1434 - val_accuracy: 0.8997\n",
      "Epoch 44/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1367.2969 - accuracy: 0.8993 - val_loss: 2185.7856 - val_accuracy: 0.8851\n",
      "Epoch 45/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1388.4017 - accuracy: 0.8981 - val_loss: 2165.6841 - val_accuracy: 0.8934\n",
      "Epoch 46/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1393.8817 - accuracy: 0.8989 - val_loss: 2255.5527 - val_accuracy: 0.8857\n",
      "Epoch 47/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1387.3762 - accuracy: 0.8989 - val_loss: 2048.1902 - val_accuracy: 0.8944\n",
      "Epoch 48/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1374.9966 - accuracy: 0.9005 - val_loss: 1992.0068 - val_accuracy: 0.8943\n",
      "Epoch 49/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1370.0724 - accuracy: 0.8996 - val_loss: 2067.9900 - val_accuracy: 0.8941\n",
      "Epoch 50/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1336.8864 - accuracy: 0.9011 - val_loss: 2300.5190 - val_accuracy: 0.8855\n",
      "Epoch 51/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1350.7236 - accuracy: 0.8997 - val_loss: 2294.8804 - val_accuracy: 0.8765\n",
      "Epoch 52/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1378.6746 - accuracy: 0.8992 - val_loss: 2332.0879 - val_accuracy: 0.8693\n",
      "Epoch 53/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1392.8910 - accuracy: 0.8986 - val_loss: 2195.2461 - val_accuracy: 0.8843\n",
      "Epoch 54/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1348.0914 - accuracy: 0.9018 - val_loss: 1963.9186 - val_accuracy: 0.8944\n",
      "Epoch 55/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1385.0815 - accuracy: 0.8988 - val_loss: 2181.0952 - val_accuracy: 0.8852\n",
      "Epoch 56/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1361.9136 - accuracy: 0.8992 - val_loss: 2243.3850 - val_accuracy: 0.8892\n",
      "Epoch 57/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1361.0723 - accuracy: 0.8992 - val_loss: 2183.2893 - val_accuracy: 0.8932\n",
      "Epoch 58/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1382.9564 - accuracy: 0.8998 - val_loss: 2264.9597 - val_accuracy: 0.8781\n",
      "Epoch 59/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1364.1674 - accuracy: 0.8994 - val_loss: 2104.5859 - val_accuracy: 0.8950\n",
      "Epoch 60/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1381.8788 - accuracy: 0.8995 - val_loss: 2041.2876 - val_accuracy: 0.9019\n",
      "Epoch 61/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1414.4159 - accuracy: 0.8985 - val_loss: 2292.6677 - val_accuracy: 0.8837\n",
      "Epoch 62/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1453.7690 - accuracy: 0.8982 - val_loss: 2193.3457 - val_accuracy: 0.8919\n",
      "Epoch 63/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1349.3214 - accuracy: 0.9000 - val_loss: 2427.4509 - val_accuracy: 0.8672\n",
      "Epoch 64/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1392.7719 - accuracy: 0.8984 - val_loss: 2292.4099 - val_accuracy: 0.8915\n",
      "Epoch 65/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1343.9742 - accuracy: 0.9003 - val_loss: 2125.0281 - val_accuracy: 0.8952\n",
      "Epoch 66/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1353.5175 - accuracy: 0.8990 - val_loss: 2105.8931 - val_accuracy: 0.8988\n",
      "Epoch 67/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1352.6709 - accuracy: 0.9001 - val_loss: 2180.7900 - val_accuracy: 0.8917\n",
      "Epoch 68/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1381.3140 - accuracy: 0.8988 - val_loss: 1997.7800 - val_accuracy: 0.8985\n",
      "Epoch 69/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1360.4841 - accuracy: 0.8997 - val_loss: 2097.9580 - val_accuracy: 0.8881\n",
      "Epoch 70/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1391.0679 - accuracy: 0.8983 - val_loss: 1955.2776 - val_accuracy: 0.9026\n",
      "Epoch 71/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1422.7638 - accuracy: 0.8982 - val_loss: 2060.9209 - val_accuracy: 0.9023\n",
      "Epoch 72/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1373.1680 - accuracy: 0.9007 - val_loss: 2386.0303 - val_accuracy: 0.8862\n",
      "Epoch 73/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1391.0635 - accuracy: 0.8981 - val_loss: 2058.0957 - val_accuracy: 0.8983\n",
      "Epoch 74/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1417.6398 - accuracy: 0.8996 - val_loss: 2272.9961 - val_accuracy: 0.8890\n",
      "Epoch 75/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1351.7894 - accuracy: 0.9008 - val_loss: 2237.3088 - val_accuracy: 0.8818\n",
      "Epoch 76/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1367.3682 - accuracy: 0.9001 - val_loss: 2140.8623 - val_accuracy: 0.8918\n",
      "Epoch 77/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1360.6752 - accuracy: 0.9002 - val_loss: 2226.0112 - val_accuracy: 0.8908\n",
      "Epoch 78/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1435.7950 - accuracy: 0.8980 - val_loss: 2078.2017 - val_accuracy: 0.8950\n",
      "Epoch 79/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1350.9463 - accuracy: 0.9007 - val_loss: 2081.2844 - val_accuracy: 0.8964\n",
      "Epoch 80/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1380.6023 - accuracy: 0.8986 - val_loss: 2171.1003 - val_accuracy: 0.8899\n",
      "Epoch 81/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1418.7805 - accuracy: 0.8995 - val_loss: 2210.7043 - val_accuracy: 0.8798\n",
      "Epoch 82/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1380.3865 - accuracy: 0.8996 - val_loss: 2337.3354 - val_accuracy: 0.8791\n",
      "Epoch 83/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1394.6969 - accuracy: 0.8995 - val_loss: 1972.4373 - val_accuracy: 0.9036\n",
      "Epoch 84/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1375.6864 - accuracy: 0.8991 - val_loss: 2588.3508 - val_accuracy: 0.8657\n",
      "Epoch 85/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1386.1875 - accuracy: 0.9001 - val_loss: 2243.5015 - val_accuracy: 0.8872\n",
      "Epoch 86/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1394.0612 - accuracy: 0.9003 - val_loss: 2162.9917 - val_accuracy: 0.8910\n",
      "Epoch 87/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1393.0532 - accuracy: 0.8992 - val_loss: 2204.3711 - val_accuracy: 0.8839\n",
      "Epoch 88/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1418.2826 - accuracy: 0.8982 - val_loss: 2320.3271 - val_accuracy: 0.8881\n",
      "Epoch 89/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1362.0768 - accuracy: 0.9015 - val_loss: 2094.5227 - val_accuracy: 0.8970\n",
      "Epoch 90/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1361.4840 - accuracy: 0.9001 - val_loss: 2157.8838 - val_accuracy: 0.8939\n",
      "Epoch 91/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1361.4879 - accuracy: 0.8998 - val_loss: 2181.6208 - val_accuracy: 0.8867\n",
      "Epoch 92/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1359.8889 - accuracy: 0.9005 - val_loss: 2210.3933 - val_accuracy: 0.8972\n",
      "Epoch 93/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1404.1049 - accuracy: 0.8988 - val_loss: 2213.0535 - val_accuracy: 0.8950\n",
      "Epoch 94/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1397.8293 - accuracy: 0.9006 - val_loss: 2284.0381 - val_accuracy: 0.8886\n",
      "Epoch 95/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1324.8599 - accuracy: 0.9000 - val_loss: 2344.0779 - val_accuracy: 0.8775\n",
      "Epoch 96/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1386.7786 - accuracy: 0.8985 - val_loss: 2874.1646 - val_accuracy: 0.8591\n",
      "Epoch 97/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1436.9036 - accuracy: 0.8978 - val_loss: 2143.3962 - val_accuracy: 0.8876\n",
      "Epoch 98/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1347.5875 - accuracy: 0.9015 - val_loss: 2062.8777 - val_accuracy: 0.9012\n",
      "Epoch 99/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1407.9205 - accuracy: 0.8982 - val_loss: 2088.7715 - val_accuracy: 0.9035\n",
      "Epoch 100/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1357.0171 - accuracy: 0.9015 - val_loss: 2101.8650 - val_accuracy: 0.8949\n",
      "Epoch 101/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1386.4023 - accuracy: 0.8993 - val_loss: 2229.5481 - val_accuracy: 0.8913\n",
      "Epoch 102/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1374.2881 - accuracy: 0.9008 - val_loss: 2288.4463 - val_accuracy: 0.8735\n",
      "Epoch 103/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1339.6268 - accuracy: 0.9006 - val_loss: 2438.5640 - val_accuracy: 0.8825\n",
      "Epoch 104/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1425.9124 - accuracy: 0.8989 - val_loss: 2098.0613 - val_accuracy: 0.8971\n",
      "Epoch 105/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1392.8218 - accuracy: 0.8999 - val_loss: 2324.7981 - val_accuracy: 0.8809\n",
      "Epoch 106/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1364.7095 - accuracy: 0.9002 - val_loss: 2005.8632 - val_accuracy: 0.8983\n",
      "Epoch 107/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1372.6244 - accuracy: 0.9003 - val_loss: 2256.3423 - val_accuracy: 0.8919\n",
      "Epoch 108/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1320.7312 - accuracy: 0.9002 - val_loss: 2125.4805 - val_accuracy: 0.8913\n",
      "Epoch 109/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1383.5667 - accuracy: 0.8989 - val_loss: 2147.8733 - val_accuracy: 0.8950\n",
      "Epoch 110/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1351.5648 - accuracy: 0.8986 - val_loss: 2376.7478 - val_accuracy: 0.8881\n",
      "Epoch 111/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1349.3655 - accuracy: 0.8995 - val_loss: 2165.5591 - val_accuracy: 0.8912\n",
      "Epoch 112/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1408.5250 - accuracy: 0.8984 - val_loss: 2681.8784 - val_accuracy: 0.8740\n",
      "Epoch 113/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1384.8335 - accuracy: 0.9009 - val_loss: 2486.2395 - val_accuracy: 0.8717\n",
      "Epoch 114/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1370.0941 - accuracy: 0.8991 - val_loss: 2045.7867 - val_accuracy: 0.8989\n",
      "Epoch 115/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1381.7721 - accuracy: 0.8997 - val_loss: 2160.7100 - val_accuracy: 0.8888\n",
      "Epoch 116/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1320.0959 - accuracy: 0.9003 - val_loss: 2275.8718 - val_accuracy: 0.8963\n",
      "Epoch 117/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1419.5883 - accuracy: 0.8973 - val_loss: 2211.0901 - val_accuracy: 0.8923\n",
      "Epoch 118/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1374.7975 - accuracy: 0.9014 - val_loss: 2188.5437 - val_accuracy: 0.8877\n",
      "Epoch 119/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1340.7070 - accuracy: 0.9008 - val_loss: 2189.5479 - val_accuracy: 0.8874\n",
      "Epoch 120/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1365.8375 - accuracy: 0.9010 - val_loss: 2177.9827 - val_accuracy: 0.8835\n",
      "Epoch 121/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1378.6935 - accuracy: 0.8986 - val_loss: 2153.0171 - val_accuracy: 0.9006\n",
      "Epoch 122/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1384.6001 - accuracy: 0.9009 - val_loss: 2164.6062 - val_accuracy: 0.8967\n",
      "Epoch 123/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1405.3109 - accuracy: 0.8986 - val_loss: 2129.5398 - val_accuracy: 0.8994\n",
      "Epoch 124/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1435.2832 - accuracy: 0.8986 - val_loss: 2144.1064 - val_accuracy: 0.8974\n",
      "Epoch 125/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1402.0371 - accuracy: 0.9001 - val_loss: 2355.3245 - val_accuracy: 0.8847\n",
      "Epoch 126/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1380.7781 - accuracy: 0.8991 - val_loss: 2474.5347 - val_accuracy: 0.8794\n",
      "Epoch 127/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1344.6917 - accuracy: 0.9025 - val_loss: 2149.2927 - val_accuracy: 0.8975\n",
      "Epoch 128/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1370.2477 - accuracy: 0.8995 - val_loss: 2300.4143 - val_accuracy: 0.8899\n",
      "Epoch 129/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1389.1935 - accuracy: 0.8997 - val_loss: 2226.6838 - val_accuracy: 0.8901\n",
      "Epoch 130/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1383.0634 - accuracy: 0.8998 - val_loss: 2291.1643 - val_accuracy: 0.8849\n",
      "Epoch 131/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1379.0328 - accuracy: 0.8997 - val_loss: 2204.5281 - val_accuracy: 0.8916\n",
      "Epoch 132/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1378.9066 - accuracy: 0.8995 - val_loss: 2220.2910 - val_accuracy: 0.8970\n",
      "Epoch 133/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1411.9556 - accuracy: 0.8993 - val_loss: 2207.0024 - val_accuracy: 0.8975\n",
      "Epoch 134/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1406.7301 - accuracy: 0.8985 - val_loss: 2443.4917 - val_accuracy: 0.8825\n",
      "Epoch 135/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1380.5820 - accuracy: 0.9007 - val_loss: 2084.8870 - val_accuracy: 0.9013\n",
      "Epoch 136/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1382.6138 - accuracy: 0.8998 - val_loss: 2172.3667 - val_accuracy: 0.8921\n",
      "Epoch 137/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1395.2251 - accuracy: 0.8996 - val_loss: 2249.8259 - val_accuracy: 0.8840\n",
      "Epoch 138/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1358.1726 - accuracy: 0.9005 - val_loss: 2263.3989 - val_accuracy: 0.8815\n",
      "Epoch 139/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1368.0586 - accuracy: 0.8988 - val_loss: 2287.5481 - val_accuracy: 0.8887\n",
      "Epoch 140/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1380.3176 - accuracy: 0.8999 - val_loss: 2164.1013 - val_accuracy: 0.8972\n",
      "Epoch 141/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1367.1364 - accuracy: 0.8992 - val_loss: 2350.7061 - val_accuracy: 0.8841\n",
      "Epoch 142/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1441.9785 - accuracy: 0.8975 - val_loss: 2323.4211 - val_accuracy: 0.8950\n",
      "Epoch 143/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1400.4199 - accuracy: 0.9005 - val_loss: 2144.1599 - val_accuracy: 0.9023\n",
      "Epoch 144/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1397.3896 - accuracy: 0.8999 - val_loss: 2694.7192 - val_accuracy: 0.8707\n",
      "Epoch 145/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1370.2688 - accuracy: 0.9005 - val_loss: 2111.1211 - val_accuracy: 0.8974\n",
      "Epoch 146/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1367.0924 - accuracy: 0.9006 - val_loss: 2006.0214 - val_accuracy: 0.9007\n",
      "Epoch 147/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1365.4572 - accuracy: 0.8994 - val_loss: 2193.1887 - val_accuracy: 0.8952\n",
      "Epoch 148/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1407.4535 - accuracy: 0.8999 - val_loss: 2597.0791 - val_accuracy: 0.8777\n",
      "Epoch 149/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1366.6350 - accuracy: 0.9005 - val_loss: 2329.7810 - val_accuracy: 0.8789\n",
      "Epoch 150/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1373.1990 - accuracy: 0.8993 - val_loss: 2505.7864 - val_accuracy: 0.8779\n",
      "Epoch 151/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1365.5358 - accuracy: 0.9003 - val_loss: 2199.5073 - val_accuracy: 0.8832\n",
      "Epoch 152/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1380.0458 - accuracy: 0.9002 - val_loss: 2096.5059 - val_accuracy: 0.8968\n",
      "Epoch 153/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1394.1346 - accuracy: 0.8988 - val_loss: 2167.6187 - val_accuracy: 0.8948\n",
      "Epoch 154/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1388.4050 - accuracy: 0.9012 - val_loss: 2109.9780 - val_accuracy: 0.8925\n",
      "Epoch 155/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1404.8889 - accuracy: 0.9002 - val_loss: 2220.8335 - val_accuracy: 0.8970\n",
      "Epoch 156/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1455.2095 - accuracy: 0.8988 - val_loss: 2642.2668 - val_accuracy: 0.8723\n",
      "Epoch 157/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1402.8575 - accuracy: 0.9013 - val_loss: 2145.6758 - val_accuracy: 0.8948\n",
      "Epoch 158/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1353.2836 - accuracy: 0.9002 - val_loss: 2220.4932 - val_accuracy: 0.8953\n",
      "Epoch 159/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1407.4446 - accuracy: 0.8990 - val_loss: 2343.9419 - val_accuracy: 0.8859\n",
      "Epoch 160/167\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1370.2850 - accuracy: 0.9000 - val_loss: 2290.0972 - val_accuracy: 0.8982\n",
      "Epoch 161/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1404.9567 - accuracy: 0.9004 - val_loss: 2440.4592 - val_accuracy: 0.8806\n",
      "Epoch 162/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1413.0857 - accuracy: 0.8997 - val_loss: 2241.2649 - val_accuracy: 0.8952\n",
      "Epoch 163/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1404.3947 - accuracy: 0.8989 - val_loss: 2391.4829 - val_accuracy: 0.8823\n",
      "Epoch 164/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1365.3713 - accuracy: 0.9001 - val_loss: 2489.1091 - val_accuracy: 0.8729\n",
      "Epoch 165/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1379.4871 - accuracy: 0.8996 - val_loss: 2411.2930 - val_accuracy: 0.8705\n",
      "Epoch 166/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1428.3270 - accuracy: 0.8985 - val_loss: 2786.6006 - val_accuracy: 0.8621\n",
      "Epoch 167/167\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 1382.2242 - accuracy: 0.9003 - val_loss: 2188.7461 - val_accuracy: 0.8886\n",
      "Test loss: 2188.74609375\n",
      "Test accuracy: 0.8885999917984009\n"
     ]
    }
   ],
   "source": [
    "# 2. Define cost/loss function and compile the model\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=optimizers.Adam(learning_rate=0.5),  \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 3. Fitting the model\n",
    "#     - data: 60,000\n",
    "#     - batchsize (per one iteration): 100\n",
    "#     - data contains 600 batches (i.e 600 iterations to cover each datapoint)\n",
    "#     - 10,000 iterations => 167 epochs\n",
    "print(\"ACHTUNG: In what follows:\")\n",
    "print(\"accuracy: percentage of correctly classified (training) data\")\n",
    "print(\"val_accuracy: percentage of correctly classified (validation) data\")\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=100,\n",
    "          epochs=167,\n",
    "          verbose=1,\n",
    "          validation_data=(X_holdout, y_holdout),\n",
    "          # validation_split=0.1,\n",
    "         )\n",
    "\n",
    "score = model.evaluate(X_holdout, y_holdout, verbose=0)\n",
    "print('Final validation loss:', score[0])\n",
    "print('Final validation accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd345bb0-2525-42ce-b968-8ff48da83154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
